{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Stackoverflow QA",
   "id": "fe27fe04dde386f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T05:29:57.603024Z",
     "start_time": "2025-10-17T04:44:44.159443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stackapi import StackAPI\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "def clean_html(raw_html):\n",
    "    \"\"\"Remove HTML tags and unescape basic entities.\"\"\"\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    cleantext = cleantext.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')\n",
    "    return cleantext.strip()\n",
    "\n",
    "# --- StackOverflow API setup ---\n",
    "SITE = StackAPI('stackoverflow', key='rl_XSrg9NN8mZuYZNRcthYeLE1hk')\n",
    "SITE.page_size = 100           # items per page\n",
    "SITE.max_pages = 50            # max pages to fetch\n",
    "SITE.sleep_between_requests = 1  # be kind to the API\n",
    "\n",
    "# --- File setup ---\n",
    "output_path = '../data/ue_qa_raw.json'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "questions_data = []\n",
    "page = 1\n",
    "has_more = True\n",
    "\n",
    "# If the file exists, resume from where you left off\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        questions_data = json.load(f)\n",
    "    print(f\"Resuming from {len(questions_data)} saved Q&A pairs.\")\n",
    "\n",
    "# --- Main loop ---\n",
    "while has_more and page <= SITE.max_pages:\n",
    "    print(f\"\\nüîπ Fetching page {page}...\")\n",
    "    resp = SITE.fetch(\n",
    "        'questions',\n",
    "        tagged='unreal-engine4',\n",
    "        filter='withbody',\n",
    "        page=page,\n",
    "        sort='creation'\n",
    "    )\n",
    "\n",
    "    for question in resp['items']:\n",
    "        qid = question['question_id']\n",
    "        title = clean_html(question['title'])\n",
    "        body = clean_html(question['body'])\n",
    "        answers = []\n",
    "\n",
    "        try:\n",
    "            a = SITE.fetch(f'questions/{qid}/answers', filter='withbody')\n",
    "            for ans in a['items']:\n",
    "                answers.append(clean_html(ans['body']))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching answers for {qid}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if answers:\n",
    "            questions_data.append({\n",
    "                \"question_id\": qid,\n",
    "                \"question\": title,\n",
    "                \"question_body\": body,\n",
    "                \"answers\": answers\n",
    "            })\n",
    "\n",
    "    # Save progress every 5 pages\n",
    "    if page % 5 == 0:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(questions_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"üíæ Saved {len(questions_data)} Q&A pairs so far...\")\n",
    "\n",
    "    has_more = resp.get('has_more', False)\n",
    "    page += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "# --- Final save ---\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(questions_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Done! Total collected: {len(questions_data)} Q&A pairs\")\n",
    "print(f\"üìÅ Saved to: {output_path}\")\n"
   ],
   "id": "4492a84f513f0188",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from 661 saved Q&A pairs.\n",
      "\n",
      "üîπ Fetching page 1...\n",
      "‚ö†Ô∏è Error fetching answers for 58428978: ('https://api.stackexchange.com/2.3/questions/58428978/answers/?pagesize=100&page=1&filter=withbody&key=rl_XSrg9NN8mZuYZNRcthYeLE1hk&site=stackoverflow', 'Expecting value: line 1 column 1 (char 0)', 'Expecting value: line 1 column 1 (char 0)', 'Expecting value: line 1 column 1 (char 0)')\n",
      "\n",
      "‚úÖ Done! Total collected: 2483 Q&A pairs\n",
      "üìÅ Saved to: ../data/ue_qa_raw.json\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T05:36:18.893129Z",
     "start_time": "2025-10-17T05:36:18.296077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('../data/ue_qa_raw.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for item in data:\n",
    "    question = BeautifulSoup(item['question_body'], 'html.parser').get_text()\n",
    "    answers = [BeautifulSoup(a, 'html.parser').get_text() for a in item['answers']]\n",
    "\n",
    "    for ans in answers:\n",
    "        dataset.append({\n",
    "            \"instruction\": item['question'],\n",
    "            \"input\": question,\n",
    "            \"output\": ans\n",
    "        })\n",
    "\n",
    "with open('../data/ue_finetune_dataset.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(dataset, indent=2, ensure_ascii=False))\n",
    "    \"\"\"for d in dataset:\n",
    "        f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\"\"\"\n"
   ],
   "id": "c51e751074848f71",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_392066/466673404.py:12: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  answers = [BeautifulSoup(a, 'html.parser').get_text() for a in item['answers']]\n",
      "/tmp/ipykernel_392066/466673404.py:11: XMLParsedAsHTMLWarning: It looks like you're using an HTML parser to parse an XML document.\n",
      "\n",
      "Assuming this really is an XML document, what you're doing might work, but you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the Python package 'lxml' installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "\n",
      "If you want or need to use an HTML parser on this document, you can make this warning go away by filtering it. To do that, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import XMLParsedAsHTMLWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
      "\n",
      "  question = BeautifulSoup(item['question_body'], 'html.parser').get_text()\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Parse UE documentation",
   "id": "1b2061e379516677"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-10-15T08:46:17.598116Z",
     "start_time": "2025-10-15T08:46:17.592031Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DOC_PATH = Path(\"/path/to/UnrealEngine/Engine/Documentation/HTML/en\")\n",
    "output = []\n",
    "\n",
    "for html_file in DOC_PATH.rglob(\"*.html\"):\n",
    "    try:\n",
    "        with open(html_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            soup = BeautifulSoup(f, \"html.parser\")\n",
    "        title = soup.title.string if soup.title else html_file.stem\n",
    "        # Many Unreal docs wrap the actual text in <div class=\"doc-content\"> or <article>\n",
    "        article = soup.select_one(\".doc-content\") or soup.select_one(\"article\") or soup.body\n",
    "        text = article.get_text(separator=\"\\n\", strip=True) if article else \"\"\n",
    "        output.append({\"file\": str(html_file), \"title\": title, \"content\": text})\n",
    "        print(\"‚úÖ Parsed\", html_file)\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error\", html_file, e)\n",
    "\n",
    "with open(\"ue_docs_local.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for o in output:\n",
    "        f.write(json.dumps(o, ensure_ascii=False) + \"\\n\")\n",
    "    f.close()\n"
   ],
   "id": "7b573aa9c1028bb1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
